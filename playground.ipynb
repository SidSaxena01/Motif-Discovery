{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import crepe\n",
    "import mir_eval\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import plot, show, figure, imshow\n",
    "plt.rcParams['figure.figsize'] = (15, 6)\n",
    "\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import essentia.standard as es\n",
    "\n",
    "audiofile = '/Users/fernando/dev/upf/mir/motif-detection/John Williams & London Symphony Orchestra - Star Wars - The Ultimate Digital Collection (2016 - Soundtracks) [Flac 24-44_192]/46. John Williams & London Symphony Orchestra - Episode IV - Main Title.flac'\n",
    "audiofile = \"/Users/fernando/Downloads/other.flac\"\n",
    "sr = 44100\n",
    "# Load audio file.\n",
    "# It is recommended to apply equal-loudness filter for PredominantPitchMelodia.\n",
    "loader = es.EqloudLoader(filename=audiofile, sampleRate=sr)\n",
    "audio = loader()\n",
    "print(\"Duration of the audio sample [sec]:\")\n",
    "print(len(audio)/sr)\n",
    "\n",
    "# Extract the pitch curve\n",
    "# PitchMelodia takes the entire audio signal as input (no frame-wise processing is required).\n",
    "\n",
    "pitch_extractor = es.PredominantPitchMelodia(frameSize=2048, hopSize=128)\n",
    "pitch_values, pitch_confidence = pitch_extractor(audio)\n",
    "\n",
    "# Pitch is estimated on frames. Compute frame time positions.\n",
    "pitch_times = numpy.linspace(0.0,len(audio)/sr,len(pitch_values) )\n",
    "\n",
    "# Plot the estimated pitch contour and confidence over time.\n",
    "f, axarr = plt.subplots(2, sharex=True)\n",
    "axarr[0].plot(pitch_times, pitch_values)\n",
    "axarr[0].set_title('estimated pitch [Hz]')\n",
    "axarr[1].plot(pitch_times, pitch_confidence)\n",
    "axarr[1].set_title('pitch confidence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play the audio\n",
    "IPython.display.Audio(audio, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_seconds = len(audio) / float(sr)\n",
    "pitch_times = np.linspace(0.0, duration_seconds, len(pitch_values))\n",
    "\n",
    "# # 4) Optionally apply a confidence threshold: set pitch to 0 where confidence is low\n",
    "# if confidence_threshold is not None:\n",
    "#     pitch_values[pitch_confidence < confidence_threshold] = 0.0\n",
    "\n",
    "# 5) Use mir_eval's sonify.pitch_contour to create a sine wave at the detected pitches\n",
    "sonification = mir_eval.sonify.pitch_contour(\n",
    "    pitch_times,  # timestamps for each pitch frame\n",
    "    pitch_values, # pitch in Hz\n",
    "    fs=sr         # sample rate for the synthesized signal\n",
    ")\n",
    "\n",
    "# 6) Return an IPython.display.Audio object so you can listen to it directly in a notebook\n",
    "Audio(sonification, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pitch(\n",
    "    audio_path: str,\n",
    "    # voicing_threshold: float = 0.3,\n",
    "    use_viterbi: bool = False,\n",
    "    model_capacity=\"full\",\n",
    "    crepe_verbose_level=1,\n",
    "    seconds_to_analyze=None,\n",
    "    resample_sr=44100,\n",
    "):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    \n",
    "    if sr != resample_sr:\n",
    "        y = librosa.resample(y, orig_sr=sr, target_sr=resample_sr)\n",
    "        sr = resample_sr\n",
    "\n",
    "    if seconds_to_analyze:\n",
    "        y = y[: int(sr * seconds_to_analyze)]\n",
    "    time, frequency, confidence, activation = crepe.predict(\n",
    "        audio=y,\n",
    "        sr=sr,\n",
    "        viterbi=use_viterbi,\n",
    "        model_capacity=model_capacity,\n",
    "        verbose=crepe_verbose_level,\n",
    "    )\n",
    "\n",
    "    # for idx, conf in enumerate(confidence):\n",
    "    #     if conf < voicing_threshold:\n",
    "    #         frequency[idx] = 0\n",
    "\n",
    "    return time, frequency, confidence, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pitch(time, frequency, confidence, activation):\n",
    "    \"\"\"\n",
    "    Plot pitch tracking information including the fundamental frequency (F0) over time, \n",
    "    the confidence of the estimates, and an activation matrix representing the salience \n",
    "    of pitches over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : array_like\n",
    "        An array of time stamps at which the frequency and confidence values are estimated.\n",
    "    frequency : array_like\n",
    "        An array containing estimated fundamental frequency (F0) values in Hertz (Hz) for each time stamp.\n",
    "    confidence : array_like\n",
    "        An array containing confidence values associated with each F0 estimate.\n",
    "    activation : array_like\n",
    "        A 2D array representing the activation of different pitch bins over time. \n",
    "        The vertical dimension corresponds to pitch bins, and the horizontal dimension \n",
    "        corresponds to time.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function plots three subplots: The first subplot displays the F0 estimate over time,\n",
    "    the second subplot shows the confidence of these estimates over time, and the third \n",
    "    subplot shows the activation matrix with pitch bins in cents over time. A bug fix is \n",
    "    applied for the pitch calculation as per a known issue in the CREPE repository.\n",
    "\n",
    "    The function does not return any values but renders a matplotlib figure directly.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] https://github.com/marl/crepe/issues/2\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(ncols=1, nrows=3, figsize=(12, 8), sharex=False)\n",
    "    axes[0].plot(time, frequency)\n",
    "    axes[0].set_xlabel(\"Time (s)\")\n",
    "    axes[0].set_ylabel(\"Estimated F0 (Hz)\")\n",
    "    axes[0].set_title(\"F0 Estimate Over Time\")\n",
    "    \n",
    "    axes[1].plot(time, confidence)\n",
    "    axes[1].set_xlabel(\"Time (s)\")\n",
    "    axes[1].set_ylabel(\"Confidence\")\n",
    "    axes[1].set_title(\"Estimate Confidence Over Time\")\n",
    "    \n",
    "    axes[2].imshow(activation.T, origin=\"lower\", aspect=\"auto\")\n",
    "    axes[2].set_xticks(np.arange(len(activation))[::500])\n",
    "    \n",
    "    c1 = 32.7 # Hz, fix for a known issue in CREPE\n",
    "    c1_cent = mir_eval.melody.hz2cents(np.array([c1]))[0]\n",
    "    c = np.arange(0, 360) * 20 + c1_cent\n",
    "    freq = 10 * 2 ** (c / 1200)\n",
    "    \n",
    "    axes[2].set_yticks(np.arange(len(freq))[::35])\n",
    "    axes[2].set_yticklabels([int(f) for f in freq[::35]])\n",
    "    axes[2].set_ylim([0, 300])\n",
    "    axes[2].set_xticklabels((np.arange(len(activation))[::500] / 100).astype(int))\n",
    "    axes[2].set_xlabel(\"Time (s)\")\n",
    "    axes[2].set_ylabel(\"Frequency\")\n",
    "    axes[2].set_title(\"Activation Matrix: 20 Cent Bins Over Time\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 44100\n",
    "time, frequency, confidence, activation = estimate_pitch(audiofile, use_viterbi=True, resample_sr=sr)\n",
    "plot_pitch(time, frequency, confidence, activation)\n",
    "\n",
    "sonification = mir_eval.sonify.pitch_contour(time, frequency, fs=sr)\n",
    "display(Audio(sonification, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 44100\n",
    "sonification = mir_eval.sonify.pitch_contour(time, frequency, fs=sr)\n",
    "display(Audio(sonification, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "motif-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
